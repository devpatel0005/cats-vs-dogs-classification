{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceType":"datasetVersion","sourceId":1122723,"datasetId":630856,"databundleVersionId":1153125}],"dockerImageVersionId":31287,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-28T13:18:44.090011Z","iopub.execute_input":"2026-02-28T13:18:44.090192Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/datasets/karakaggle/kaggle-cat-vs-dog-dataset/kagglecatsanddogs_3367a/readme[1].txt\n/kaggle/input/datasets/karakaggle/kaggle-cat-vs-dog-dataset/kagglecatsanddogs_3367a/MSR-LA - 3467.docx\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom keras.layers import Dropout,Dense,BatchNormalization,MaxPooling2D,Conv2D,Flatten","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# THe image_dataset_from_directory is the best method comapred to datagen.flow_from_directory si\n# beacuse it is executed through tensorflow and it is good for large datasets and is fast enoough\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_ds=keras.utils.image_dataset_from_directory(\n    directory='/kaggle/input/datasets/karakaggle/kaggle-cat-vs-dog-dataset/kagglecatsanddogs_3367a/PetImages',\n    labels='inferred', # means include labels\n    subset='training', # used this data for training\n    label_mode='int',  # label must in integer format\n    seed=42, # random seed so that the train and test data does not match\n    validation_split=0.2, # for training and validaiton data\n    batch_size=32,\n    image_size=(256,256)    \n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_ds.class_names","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_ds=keras.utils.image_dataset_from_directory(\n    directory='/kaggle/input/datasets/karakaggle/kaggle-cat-vs-dog-dataset/kagglecatsanddogs_3367a/PetImages',\n    labels='inferred',\n    subset='validation',\n    label_mode='int',\n    seed=42,\n    validation_split=0.2,\n    batch_size=32,\n    image_size=(256,256)    \n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_ds.class_names # This method is used to see the clases in the data","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Now we will normalize the Data","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def normalize(data,label): #since images pixels are from 0 to max 255 we will divide with 255 to normalize\n    new=tf.cast(data/255,tf.float32) # The cast method of tf is  used to change the datatpye of the data\n    return new,label\n\ntrain_df=train_ds.map(normalize)\ntest_df=test_ds.map(normalize) # Here labelled data is there so we passed label as parameter","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.element_spec # method to get the input dimensions","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CNN Architecture","metadata":{}},{"cell_type":"code","source":"model=Sequential(\n    #tf.keras.layers.Rescaling(1.0/255) by this also we can scale the data\n)\n\n#1st convolutional layer\nmodel.add(Conv2D(32,kernel_size=(4,4),padding='same',activation='relu',input_shape=(256,256,3)))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(3,3),strides=2,padding='same'))\n\nmodel.add(Conv2D(64,kernel_size=(4,4),padding='same',activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(3,3),strides=2,padding='same'))\n\n\nmodel.add(Conv2D(128,kernel_size=(4,4),padding='same',activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(3,3),strides=2,padding='same'))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(64,activation='relu'))\nmodel.add(Dense(32,activation='relu'))\nmodel.add(Dense(1,activation='sigmoid'))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.compile(optimizer=Adam(0.001),loss='binary_crossentropy',metrics=['accuracy'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history=model.fit(train_df,epochs=50,batch_size=64,validation_data=test_df,verbose=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot(history.history('loss'))\nplt.plot(history.history('val_loss'))\nplt.legend()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}